# Flexible Catalog â€“ JSON + Virtual Columns ğŸš€

A **product catalog** where *JSON columns* hold dynamic attributes, exposed by **virtual/persistent columns** for indexing and fast filters â€” giving the agility of **semi-structured data** with the performance of relational indexes.

**Problem**: e-commerce and marketplace catalogs have many product-specific attributes (RAM, GPU, screen_size, battery_mAh, etc.) which vary by category. Storing every attribute as a column is inflexible; storing a blob of JSON is flexible but usually performs poorly for filters unless indexed.

**Solution**: Keep a JSON column (specs) for flexible attributes. Expose a selected set of frequently-filtered paths as virtual (generated) columns and add indexes on them (BTREE) or persistent indexes to speed up range/eq searches. Keep JSON for everything else.

This repository is a full example: database schema (`schema.sql`), a data generator to produce 300k rows (`load_data.py`), a small Express API (`/api`) that prefers generated columns for filters, and a Docker Compose setup to run everything locally.

---

This project solves real-world problems:

* Handling dynamic product attributes without rigid schema changes.
* Enabling fast filtering and sorting on frequently queried fields.
* Scaling data ingestion for large datasets (300k+ rows).
* Maintaining data integrity and validation even with semi-structured JSON.
* Providing an API that safely exposes complex queries while preventing injection or malformed requests.

---

# Built with:

MariaDB / MySQL 8.0+ (JSON support + generated columns)

Node.js + Express (API with validation + metrics)

Docker Compose (for reproducible setup)

Python (Faker) for synthetic data generation

---

## ğŸŒŸ Project Overview

Hybrid database design where:

* Product attributes (RAM, GPU, screen size, battery, etc.) are stored as **JSON**.
* Frequently queried attributes are extracted as **generated columns** for **B-tree indexing**.
* Full API layer enables **complex filters** (e.g., `ram >= 16`, `gpu = "RTX 4070"`).
* Supports high-volume data ingestion efficiently via SQL inserts or CSV + `LOAD DATA INFILE`.
* Provides built-in **analytics views** and constraints for robust data validation.

---

## ğŸ—ï¸ Project Structure

```
flexible-catalog/
â”‚
â”œâ”€â”€ schema.sql             # Database schema: tables, indexes, triggers, views, constraints
â”œâ”€â”€ load_data.py           # Python data generator for 300k+ rows
â”œâ”€â”€ products_inserts.sql   # SQL inserts generated by load_data.py
â”œâ”€â”€ products.csv           # CSV for fast bulk load
â”œâ”€â”€ docker-compose.yml     # Docker orchestration for DB + API
â”œâ”€â”€ .env.example           # Environment variable template
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ server.js          # Node.js + Express API server
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ middleware/
â”‚       â””â”€â”€ validateFilter.js   # Filter validation middleware
â”‚
â””â”€â”€ README.md              # Documentation
```

---

## âš™ï¸ Requirements & Versions

* **Docker** 20.10+ & **Docker Compose** 1.29+ (container orchestration)
* **MariaDB** 10.6+ or **MySQL** 8.0+ (JSON support, generated columns, CHECK constraints)
* **Node.js** v18+ (API server)
* **Python** 3.9+ (data generator with `faker` library)
* **NPM / Yarn** (API dependencies)

Optional:

* **Postman** or **curl** for testing API endpoints
* **Prometheus / Grafana** for monitoring

---

## ğŸš€ Getting Started

### 1. Clone Repository

```bash
git clone https://github.com/your-username/flexible-catalog.git
cd flexible-catalog
```

### 2. Configure Environment

Copy `.env.example` â†’ `.env` and set database/API credentials:

```
DB_HOST=db
DB_PORT=3306
DB_USER=root
DB_PASS=root
DB_NAME=catalog
PORT=3000
```

### 3. Start Services via Docker

```bash
docker-compose up --build
```

API: [http://localhost:3000](http://localhost:3000)
DB: localhost:3306

### 4. Load Database Schema

```bash
mysql -h127.0.0.1 -P3306 -u root -proot < schema.sql
```

### 5. Generate and Load Data

```bash
python3 load_data.py
```

* Produces `products_inserts.sql` and `products.csv`

**Option A â€“ Fast CSV Import**

```sql
LOAD DATA INFILE '/absolute/path/products.csv'
INTO TABLE products
FIELDS TERMINATED BY ',' ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 ROWS
(name, category, price, specs);
```

**Option B â€“ SQL Inserts (slower)**

```bash
mysql -h127.0.0.1 -P3306 -u root -proot < products_inserts.sql
```

---

## ğŸ“¡ API Usage

### Base URL

```
http://localhost:3000
```

### Endpoints

**Search Products â€“ filter by attributes:**

```http
POST /products/search
Content-Type: application/json
{
  "filters": [
    { "field": "ram", "op": ">=", "value": 16 },
    { "field": "gpu", "op": "=", "value": "RTX 4070" }
  ],
  "page": 1,
  "pageSize": 20
}
```

### Example Response

```json
[
  {
    "id": 101,
    "name": "laptop-101",
    "category": "laptop",
    "price": 1299.99,
    "ram_gb": 16,
    "gpu": "RTX 4070"
  }
]
```

---

## ğŸ“Š Benchmarking

* JSON-only query:

```sql
SELECT * FROM products WHERE JSON_EXTRACT(specs, '$.ram') >= 16;
```

* Virtual-column indexed query (fast):

```sql
SELECT * FROM products WHERE ram_gb >= 16;
```

Virtual columns: **3â€“10Ã— faster** on 300k+ rows.

---

## ğŸ› ï¸ Advanced Features & Technical Details

* **Constraints**: JSON validation, numeric fields, positive price.
* **Triggers**: normalize JSON on insert, auto-update timestamps.
* **Views**: `product_summary` for analytics.
* **Middleware**: validates filters before hitting DB.
* **Logging**: query execution times via `morgan`.
* **Healthchecks**: Docker ensures service recovery.
* **Scalable Data Ingestion**: CSV bulk load for large datasets.

**Problems Solved:**

* Flexible schema for dynamic attributes.
* Fast search/filtering on common fields.
* Data consistency in semi-structured datasets.
* Safe API interface preventing invalid queries.

---

## ğŸ§ª Troubleshooting & Fallbacks

* **CSV Import Error** â†’ check MySQL `secure_file_priv`. Ensure file path is correct and accessible.
* **Docker container restarts** â†’ check `.env` DB settings; verify ports and credentials.
* **Slow inserts** â†’ prefer CSV import over SQL inserts.
* **API returns errors** â†’ verify request payload and filters; check server logs.
* **Database connection issues** â†’ ensure DB container is running and accessible; retry connection.
* **Schema load fails** â†’ check MySQL version compatibility (8.0+ or MariaDB 10.6+).

For persistent issues, consult logs in Docker containers (`docker logs <container>`) and ensure all dependencies meet version requirements.

---

## ğŸ“ Next Steps / Stretch Goals

* Full-text search for JSON specs.
* GraphQL API support.
* Compare PostgreSQL JSONB vs MySQL JSON for performance.
* Prometheus/Grafana for monitoring.

---

ğŸ”¥ Demonstrates **real-world DB optimization**, **scalable ingestion**, **API best practices**, solving **semi-structured data challenges**, and includes guidance for handling failures.
