# Flexible Catalog — JSON + Virtual Columns

A product catalog where JSON columns hold dynamic attributes, exposed by **virtual/persistent columns** for indexing and fast filters — giving the agility of semi-structured data with the performance of relational indexes.

This repository is a full example: database schema (`schema.sql`), a data generator to produce 300k rows (`load_data.py`), a small Express API (`/api`) that prefers generated columns for filters, and a Docker Compose setup to run everything locally.

---

**Problem**: e-commerce and marketplace catalogs have many product-specific attributes (RAM, GPU, screen_size, battery_mAh, etc.) which vary by category. Storing every attribute as a column is inflexible; storing a blob of JSON is flexible but usually performs poorly for filters unless indexed.

**Solution**: Keep a JSON column (specs) for flexible attributes. Expose a selected set of frequently-filtered paths as virtual (generated) columns and add indexes on them (BTREE) or persistent indexes to speed up range/eq searches. Keep JSON for everything else.


---


## Contents

```
flexible-catalog/
├── README.md               <-- this file
├── schema.sql              <-- DDL for MySQL (creates products table + generated columns + indexes)
├── load_data.py            <-- Python script to generate products_inserts.sql (300k rows)
├── products_inserts.sql    <-- generated by load_data.py (not included by default)
├── docker-compose.yml      <-- spins up MySQL + API
└── api/
    ├── package.json
    ├── server.js
    └── Dockerfile
```

---

## Project goals & features

* Keep product attributes flexible using a `specs JSON` column.
* Promote hot JSON paths to generated columns (stored) so you can create indexes on them — combining schema flexibility and query performance.
* Provide an API that constructs SQL using the indexed generated columns when possible, and falls back to `JSON_EXTRACT()` when a filter targets an unindexed key.
* Provide a data load script to create a large dataset (300k rows) for benchmarking.
* Provide a blueprint for adding JSON validation constraints and production-ready optimizations.

---

## Requirements / Supported Versions

Recommended environment for running the full stack via Docker:

* Docker: 20.x or later
* Docker Compose: v2.x (the `docker compose` plugin or `docker-compose` v1.29+ works)
* MySQL: 8.0 (recommended) — JSON functions, generated columns, and `CHECK` constraints are used. MariaDB may need small syntax changes.
* Node.js: 18+ (for local dev if not using Docker)
* Python: 3.8+ (to run `load_data.py`); you'll need the `faker` package (`pip install faker`)
* Optional: MySQL client (`mysql`) on host to import SQL files easily

Notes:

* The `schema.sql` uses `JSON_UNQUOTE(JSON_EXTRACT(...))` and stored generated columns. MySQL 8.0+ is the cleanest target. MariaDB has JSON and generated columns but differs in some JSON functions; you may need to adjust DDL.
* If you intend to use `CHECK` constraints on JSON paths, use MySQL 8.0+ since older versions and some MariaDB versions behave differently.

---

## Quickstart — Clone, build, run (Docker)

1. Clone the repo (replace the URL with your repository or unzip provided archive):

```bash
git clone https://github.com/<your-username>/flexible-catalog.git
cd flexible-catalog
```

2. Build and start services with Docker Compose:

```bash
docker compose up --build -d
```

This will start two services:

* `db` → MySQL 8.0, listening on port `3306` (host: `127.0.0.1:3306`) with root password `root`.
* `api` → Node/Express app, listening on port `3000`.

3. Initialize the database schema

If you have `mysql` client on the host:

```bash
# generate the inserts file before importing (next step) or use provided SQL
python3 load_data.py   # -> creates products_inserts.sql (300k rows)

# import schema
mysql -h127.0.0.1 -P3306 -u root -proot < schema.sql

# import the data (this may take several minutes)
mysql -h127.0.0.1 -P3306 -u root -proot < products_inserts.sql
```

If you *don't* have a host mysql client, you can also import by copying files into the DB container and running the mysql client there. Example (replace container name if needed):

```bash
CONTAINER=$(docker ps --filter "ancestor=mysql:8.0" --format "{{.ID}}")
docker cp schema.sql ${CONTAINER}:/schema.sql
docker cp products_inserts.sql ${CONTAINER}:/products_inserts.sql
docker exec -it ${CONTAINER} bash -c "mysql -u root -proot < /schema.sql && mysql -u root -proot < /products_inserts.sql"
```

> Tip: importing 300k INSERT statements can be slow. Consider converting into a `LOAD DATA` CSV workflow for faster ingestion or run the inserts in transactions/batches.

4. Try the API

The API runs on `http://localhost:3000`.

Example search (curl):

```bash
curl -X POST 'http://localhost:3000/products/search' \
  -H 'Content-Type: application/json' \
  -d '{
    "filters": [
      {"field":"ram", "op": ">=", "value": 16},
      {"field":"gpu", "op": "=", "value": "RTX 4070"}
    ],
    "page": 1,
    "pageSize": 10
  }'
```

Response format:

```json
{ "rows": [ { "id": 1, "name": "abc-1", "category": "laptop", "price": 1299.99, "specs": {"ram":32, "gpu":"RTX 4070", ...} }, ... ] }
```

---

## Running the API locally (without Docker)

1. Ensure MySQL is running and `schema.sql` has been imported.
2. In `api/server.js`, change the DB host from `'db'` to `'127.0.0.1'` or set environment variables (see notes below).
3. Install dependencies and start:

```bash
cd api
npm install
npm start
```

You can also modify `server.js` to read DB configuration from environment variables for portability, e.g. `process.env.DB_HOST`.

---

## Files of interest

* `schema.sql` — Creates `products` table with `specs JSON` and generated columns: `ram_gb`, `gpu`, `screen_in`, `battery_mah`. Indexes are declared on these generated columns.
* `load_data.py` — Generates `products_inserts.sql` with 300k `INSERT` statements. Install `faker` via `pip install faker`.
* `api/server.js` — Minimal Express app with `/products/search` endpoint. It maps friendly fields to generated columns and builds safe parameterized SQL.
* `docker-compose.yml` — Brings up MySQL and API services.

---

## Benchmarking & Validation plan

To measure the advantage of generated columns and indexes vs raw JSON predicates, follow this plan:

1. **Load the data** (300k rows) into the DB but comment out the index creation if you want an initial JSON-only run.
2. Run queries that use `JSON_EXTRACT` directly and capture latency (`p50`, `p95`, `p99`) using a tool like `mysqlslap`, `wrk`, or `sysbench`.

Example JSON-only query:

```sql
SELECT id FROM products
WHERE CAST(JSON_EXTRACT(specs,'$.ram') AS UNSIGNED) >= 16
  AND JSON_UNQUOTE(JSON_EXTRACT(specs,'$.gpu')) = 'RTX 4070'
LIMIT 100;
```

3. Create the generated columns + indexes (or use full `schema.sql`) and re-run the *same* queries but targeting generated columns:

```sql
SELECT id FROM products WHERE ram_gb >= 16 AND gpu = 'RTX 4070' LIMIT 100;
```

4. Use `EXPLAIN` / `EXPLAIN FORMAT=JSON` to verify index usage. Use `EXPLAIN ANALYZE` (MySQL 8.0.18+) for actual runtime metrics.

5. Record results and compare.

---

## Schema notes & gotchas

* We use **STORED** generated columns so values are computed on write and stored on disk — making indexes efficient. Some MySQL versions allow indexing VIRTUAL columns too; behavior varies.
* `JSON_UNQUOTE(JSON_EXTRACT(...))` is used to extract scalar values. For numerical columns the DB will coerce the string to the integer typed for the generated column. If you run into type issues, use `CAST(JSON_EXTRACT(... ) AS UNSIGNED)` explicitly.
* When running in MariaDB, check JSON function compatibility and adjust DDL accordingly.
* If importing huge SQL files, increase `max_allowed_packet`, or use `LOAD DATA` for CSV bulk loads.

---

## Example: extend schema with CHECK constraint

```sql
ALTER TABLE products
ADD CONSTRAINT chk_ram_nonneg CHECK (JSON_EXTRACT(specs,'$.ram') IS NOT NULL AND CAST(JSON_EXTRACT(specs,'$.ram') AS UNSIGNED) >= 0);
```

Note: `CHECK` constraints behavior differs across storage engines and versions — test on your target MySQL/MariaDB version.

---

## Troubleshooting

* **API cannot connect to DB**: If running API in Docker Compose, the host must be `db` (the service name). If running API locally, use `127.0.0.1` and ensure MySQL listens on `0.0.0.0` and port `3306`.
* **Inserts are slow**: Convert the generator to produce `LOAD DATA` CSV or execute inserts inside a transaction with batched multi-row inserts.
* **Indexes not used**: Run `EXPLAIN` on the slow queries. Make sure predicates target the generated/stored columns rather than JSON\_EXTRACT.
* **JSON path mismatch**: The generator produces keys like `ram`, `gpu`, `screen_inches`. Make sure your ingest process writes these exact paths.

---

## Next steps & extensions

* Add an admin UI to map hot JSON keys to generated columns and create indexes automatically.
* Add telemetry to monitor which JSON paths are frequently filtered so you can promote them to indexed columns.
* Replace the naive INSERT generator with a `LOAD DATA INFILE` workflow for faster bulk ingestion.
* Add integration tests and CI pipeline for schema migration and benchmarking.

---

## Contributing

Contributions welcome — fork the repo and open a PR. If you add alternative ingestion methods (CSV/Parquet), document batch sizes and timing.

---
